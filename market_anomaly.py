# # -*- coding: utf-8 -*-
# """market anomaly.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1KXfLsBAhbgUZWo3TkuXwfcejGmoeOOeO
# """

# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LogisticRegression
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.ensemble import IsolationForest
# from sklearn.metrics import classification_report
# from sklearn.preprocessing import StandardScaler
# from sklearn.neural_network import MLPClassifier
# from sklearn.svm import SVC
# from sklearn.metrics import roc_auc_score, confusion_matrix
# from sklearn.model_selection import cross_val_score
# import os
# import joblib

# df = pd.read_csv('FinancialMarketData.xlsx-E.csv')

# print(df)

# print(df.describe())

# print(df[df['Y'] == 1].describe())

# numeric_df = df.select_dtypes(include=[np.number])
# print(numeric_df.corr())

# # Get the correlation matrix
# corr_matrix = numeric_df.corr()


# # Get the upper triangle of the correlation matrix to avoid duplicates
# upper_triangle = np.triu(corr_matrix)


# # Create a mask for the upper triangle
# mask = np.triu(np.ones_like(corr_matrix), k=1)


# # Set up the matplotlib figure
# plt.figure(figsize=(12, 8))


# # Create heatmap using seaborn
# sns.heatmap(corr_matrix,
#            mask=mask,
#            annot=False,
#            cmap='coolwarm',
#            vmin=-1,
#            vmax=1,
#            center=0)


# plt.title('Correlation Heatmap')
# plt.tight_layout()
# print(plt.show())

# correlations = numeric_df.corr() ['Y'].sort_values(ascending=False)
# print("Top feature correlations with target")
# print(correlations)

# # Select some interesting columns for comparison
# cols_to_plot = ["VIX", 'XAU BGNL', 'DXY', 'MXUS', "MXJP", "GTITL30YR"]


# # Create figure with subplots
# fig, axes = plt.subplots(1, len(cols_to_plot), figsize=(20, 6))
# fig.suptitle('Feature Distributions by Target Class (Y)')


# # Create boxplots
# for i, col in enumerate(cols_to_plot):
#    data = [df[df['Y'] == 0][col], df[df['Y'] == 1][col]]
#    axes[i].boxplot(data, labels=['Y=0', 'Y=1'])
#    axes[i].set_title(col)
#    axes[i].tick_params(axis='x', rotation=0)

# plt.tight_layout()
# plt.show()

# # Create violin plots for feature distribution comparison
# plt.figure(figsize=(20, 6))


# # Create subplots
# fig, axes = plt.subplots(1, len(cols_to_plot), figsize=(20, 6))
# fig.suptitle('Feature Distributions by Target Class (Y)')


# # Create violin plots
# for i, col in enumerate(cols_to_plot):
#    # Create violin plot
#    sns.violinplot(
#        data=df,
#        x='Y',
#        y=col,
#        ax=axes[i],
#        inner='box'  # Shows quartile boxes inside violin
#    )

#    axes[i].set_title(col)
#    axes[i].set_xlabel('Target Class')

# plt.tight_layout()
# plt.show()



# # Prepare features with more relevant indicators
# X = df[['VIX', 'XAU BGNL', 'DXY', 'MXUS', 'MXJP', 'GTITL30YR']]  # Using more features
# y = df['Y']

# # Split and scale data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_test_scaled = scaler.transform(X_test)

# # Initialize models
# models = {
#     'Logistic Regression': LogisticRegression(class_weight='balanced', random_state=42),
#     'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
#     'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42),
#     'SVM': SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)
# }

# # Train and evaluate models
# for name, model in models.items():
#     print(f"\n{name} Results:")

#     # Cross-validation
#     cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
#     print(f"Cross-validation ROC-AUC scores: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

#     # Train model
#     model.fit(X_train_scaled, y_train)
#     y_pred = model.predict(X_test_scaled)

#     # Evaluate
#     print("\nClassification Report:")
#     print(classification_report(y_test, y_pred))

#     # ROC-AUC score
#     if hasattr(model, "predict_proba"):
#         y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
#         roc_auc = roc_auc_score(y_test, y_pred_proba)
#         print(f"ROC-AUC Score: {roc_auc:.3f}")

#     # Confusion Matrix
#     conf_matrix = confusion_matrix(y_test, y_pred)
#     print("\nConfusion Matrix:")
#     print(conf_matrix)

# # Feature importance for Random Forest
# rf_model = models['Random Forest']
# feature_importance = pd.DataFrame({
#     'feature': X.columns,
#     'importance': rf_model.feature_importances_
# })
# print("\nFeature Importance:")
# print(feature_importance.sort_values('importance', ascending=False))

# # After training all models, identify the best model (Random Forest in this case)
# best_model = models['Random Forest']

# # Create a feature importance DataFrame
# feature_importance = pd.DataFrame({
#     'feature': X.columns,
#     'importance': best_model.feature_importances_
# })

# # Export the objects for use in other files
# if __name__ == "__main__":
#     # Create models directory if it doesn't exist
#     os.makedirs('api/models', exist_ok=True)

#     # Save the model, scaler, and feature importance
#     joblib.dump(best_model, 'api/models/random_forest_model.joblib')
#     joblib.dump(scaler, 'api/models/scaler.joblib')
#     joblib.dump(feature_importance, 'api/models/feature_importance.joblib')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
import os
import joblib

# Load the data
df = pd.read_csv('FinancialMarketData.xlsx-E.csv')

# Check for missing data
if df.isnull().any().any():
    df = df.dropna()  # You could also fill NaN values depending on your need

print(df)

# Descriptive statistics and target class distribution
print(df.describe())
print(df[df['Y'] == 1].describe())

# Get correlations of numerical columns
numeric_df = df.select_dtypes(include=[np.number])
corr_matrix = numeric_df.corr()

# Plot correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.show()

# Check top correlations with target variable 'Y'
correlations = numeric_df.corr()['Y'].sort_values(ascending=False)
print("Top feature correlations with target")
print(correlations)

# Select features for the model
cols_to_plot = ["VIX", 'XAU BGNL', 'DXY', 'MXUS', "MXJP", "GTITL30YR"]
X = df[cols_to_plot]
y = df['Y']

# Split and scale the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(class_weight='balanced', random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42),
    'SVM': SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)
}

# Train and evaluate models
for name, model in models.items():
    print(f"\n{name} Results:")

    # Cross-validation
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
    print(f"Cross-validation ROC-AUC scores: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

    # Train model
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    # Evaluate
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # ROC-AUC score
    if hasattr(model, "predict_proba"):
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        print(f"ROC-AUC Score: {roc_auc:.3f}")

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    print("\nConfusion Matrix:")
    print(conf_matrix)

# Feature importance for Random Forest
rf_model = models['Random Forest']
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
})
print("\nFeature Importance:")
print(feature_importance.sort_values('importance', ascending=False))

# Plot feature importances
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 6))
plt.title("Feature importances")
plt.barh(range(X.shape[1]), importances[indices], align="center")
plt.yticks(range(X.shape[1]), X.columns[indices])
plt.xlabel("Relative importance")
plt.tight_layout()
plt.show()

# Save the model, scaler, and feature importance
best_model = models['Random Forest']  # or whichever model performed best
scaler = scaler  # The scaler you used
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': best_model.feature_importances_
})

# Export objects
if __name__ == "__main__":
    os.makedirs('api/models', exist_ok=True)
    joblib.dump(best_model, 'api/models/random_forest_model.joblib')
    joblib.dump(scaler, 'api/models/scaler.joblib')
    joblib.dump(feature_importance, 'api/models/feature_importance.joblib')

    print("Model, scaler, and feature importance are saved successfully.")
